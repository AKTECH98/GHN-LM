# Benchmark 15: Mini GPT Model Configuration (Transformers Dataset)
# ~25M parameters - Alternative architecture
# Uses transformers model type (new model dataset) with GHN training parameters

model:
  model_type: "transformers"
  vocab_size: 50257
  d_model: 384
  n_layer: 6
  n_head: 6
  d_ff: 1536
  max_seq_len: 64
  p_drop: 0.1

training:
  epochs: 75
  batch_size: 64
  learning_rate: 0.0004
  weight_decay: 0.01
  warmup_steps: 750
  max_grad_norm: 1.0
  save_interval: 15
  eval_interval: 5
  log_interval: 75
  device: "cuda"
  mixed_precision: true
  gradient_accumulation_steps: 3
  seed: 42
  early_stopping_patience: 5
  early_stopping_min_delta: 0.001

data:
  seq_len: 64
  num_workers: 4

